{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dico/fr_core_news_md-3.7.0-py3-none-any.whl\n",
    "# !pip install pandas\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les traitements sont en cours, cela pourrait prendre quelques minutes...\n",
      "Nombre de doublons primaires détectés: 346730\n",
      "Nombre de doublons secondaires détectés: 976\n",
      "Fichier après suppression des doublons sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets\\trying_data_after_deduplication.csv\n",
      "Doublons supprimés sauvegardés sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets\\trying_data_duplicates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_14304\\1197112282.py:310: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_coinafrique['nb_salle_de_bain'] = pd.to_numeric(df_coinafrique['nb_salle_de_bain'], errors='coerce').astype('Int64')\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_14304\\1197112282.py:332: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_coinafrique['localisation'] = df_coinafrique['title'].apply(extract_localisation)\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_14304\\1197112282.py:333: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_coinafrique[\"type d'immobilier\"] = df_coinafrique['title'].apply(extract_logement_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier final avec les modifications sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets\\trying_data_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Charger le modèle spaCy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    folder = f'D:\\\\Bureau\\\\MemoiresStages\\\\Travaux_techniques\\\\Scrapping\\\\Datasets\\\\Juin'\n",
    "else:\n",
    "    folder = f'/mnt/d/Bureau/MemoiresStages/Travaux_techniques/Scrapping/Datasets/Juin'\n",
    "\n",
    "# Fonction pour obtenir tous les fichiers CSV dans un dossier et ses sous-dossiers\n",
    "def get_all_csv_files(folder):\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    print(\"Les traitements sont en cours, cela pourrait prendre quelques minutes...\")\n",
    "    return csv_files\n",
    "\n",
    "# Obtenir la liste de tous les fichiers CSV\n",
    "file_paths = get_all_csv_files(folder)\n",
    "\n",
    "# Charger et fusionner les fichiers\n",
    "df_list = [pd.read_csv(file) for file in file_paths]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# Convertir toutes les valeurs en minuscules pour les colonnes de type str, sauf pour les colonnes spécifiées\n",
    "columns_to_exclude = ['superficie', 'nb_pieces', 'nb_salle_de_bain', 'scraping_date', 'link']\n",
    "df = df.apply(lambda col: col.str.lower() if col.name not in columns_to_exclude and col.dtype == 'object' else col)\n",
    "\n",
    "# Filtrer les lignes en fonction de la longueur de la description\n",
    "seuil_longueur_description = 200\n",
    "df = df[df['description'].str.len() >= seuil_longueur_description]\n",
    "\n",
    "# Fonction pour extraire le nom de domaine à partir de l'URL\n",
    "def extract_site(link):\n",
    "    try:\n",
    "        return urlparse(link).netloc\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Ajouter la colonne 'site' en extrayant le nom de domaine de la colonne 'link'\n",
    "if 'link' in df.columns:\n",
    "    df['site'] = df['link'].apply(extract_site)\n",
    "\n",
    "\n",
    "# Fonction pour extraire la date à partir de 'scraping_date'\n",
    "def extract_date(scraping_date):\n",
    "    try:\n",
    "        return scraping_date.split(' ')[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Ajouter la colonne 'date' en extrayant la date de 'scraping_date'\n",
    "if 'scraping_date' in df.columns:\n",
    "    df['date'] = df['scraping_date'].apply(extract_date)\n",
    "\n",
    "# Convertir la colonne 'date' au format datetime\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "\n",
    "# Supprimer les lignes où 'price' contient des indications de paiement journalier\n",
    "df = df[~df['price'].str.contains(r'par jour|/ jour', case=False, na=False)]\n",
    "\n",
    "# Supprimer les lignes avec des prix manquants\n",
    "df = df.dropna(subset=['price'])\n",
    "\n",
    "\n",
    "# Fonction pour nettoyer et convertir les valeurs de 'price' en format numérique\n",
    "def clean_price(price):\n",
    "    # Extraire les chiffres de la chaîne de caractères\n",
    "    price = re.sub(r'[^\\d]', '', str(price))\n",
    "    # Convertir en entier\n",
    "    return float(price) if price.isdigit() else None\n",
    "\n",
    "# Appliquer la fonction de nettoyage sur la colonne 'price'\n",
    "df['price'] = df['price'].apply(clean_price)\n",
    "\n",
    "# Fonction pour nettoyer et convertir les valeurs de 'superficie' en format float\n",
    "def clean_superficie(superficie):\n",
    "    # Retirer toutes les occurrences de \"m2\" et extraire les chiffres\n",
    "    superficie = re.sub(r'\\s*m2\\s*', '', str(superficie), flags=re.IGNORECASE)\n",
    "    # Convertir en float\n",
    "    try:\n",
    "        return float(superficie)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Appliquer la fonction de nettoyage sur la colonne 'superficie' et renommer la colonne\n",
    "if 'superficie' in df.columns:\n",
    "    df['superficie_m2'] = df['superficie'].apply(clean_superficie)\n",
    "    df.drop(columns=['superficie'], inplace=True)\n",
    "\n",
    "# Fonction pour nettoyer la colonne 'nb_salle_de_bain'\n",
    "def clean_nb_salle_de_bain(nb_salle_de_bain):\n",
    "    if pd.isna(nb_salle_de_bain):\n",
    "        return None\n",
    "    # Utiliser une expression régulière pour extraire le nombre\n",
    "    match = re.search(r'\\d+', nb_salle_de_bain)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "# Appliquer la fonction à la colonne 'nb_salle_de_bain'\n",
    "df['nb_salle_de_bain'] = df['nb_salle_de_bain'].apply(clean_nb_salle_de_bain)\n",
    "\n",
    "# Fonction pour gérer les doublons primaires\n",
    "def remove_primary_duplicates(df):\n",
    "    primary_columns = [col for col in df.columns if col not in ['scraping_date', 'date']]\n",
    "    duplicates = df[df.duplicated(subset=primary_columns, keep=False)]\n",
    "    primary_duplicates_count = duplicates.shape[0]\n",
    "    print(f\"Nombre de doublons primaires détectés: {primary_duplicates_count}\")\n",
    "    df = df.drop_duplicates(subset=primary_columns, keep='first')\n",
    "    return df, duplicates\n",
    "\n",
    "# Appliquer la suppression des doublons primaires\n",
    "df, primary_duplicates = remove_primary_duplicates(df)\n",
    "\n",
    "\n",
    "# Fonction pour gérer les doublons secondaires\n",
    "def remove_secondary_duplicates(df):\n",
    "    df['description_clean'] = df['description'].apply(lambda x: re.sub(r'\\W+', ' ', str(x)))\n",
    "    duplicates = set()\n",
    "    seen = set()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if i in seen:\n",
    "            continue\n",
    "        for j, other_row in df.iterrows():\n",
    "            if i != j and j not in seen:\n",
    "                similarity = fuzz.token_set_ratio(row['description_clean'], other_row['description_clean'])\n",
    "                if similarity > 98:\n",
    "                    duplicates.add(j)\n",
    "                    seen.add(j)\n",
    "                    break  # Exit the inner loop to ensure only one duplicate is removed\n",
    "\n",
    "    secondary_duplicates_count = len(duplicates)\n",
    "    print(f\"Nombre de doublons secondaires détectés: {secondary_duplicates_count}\")\n",
    "\n",
    "    duplicates_list = list(duplicates)\n",
    "    secondary_duplicates = df.loc[duplicates_list]\n",
    "    df = df.drop(duplicates_list)\n",
    "    df.drop(columns=['description_clean'], inplace=True)\n",
    "    return df, secondary_duplicates\n",
    "\n",
    "# Appliquer la suppression des doublons secondaires\n",
    "df, secondary_duplicates = remove_secondary_duplicates(df)\n",
    "\n",
    "\n",
    "# Fusionner les doublons primaires et secondaires\n",
    "all_duplicates = pd.concat([primary_duplicates, secondary_duplicates]).drop_duplicates()\n",
    "\n",
    "# # Exporter le dataframe après la suppression des doublons\n",
    "# import platform\n",
    "# if platform.system() == 'Windows':\n",
    "#     output_path_after = f'D:\\\\Bureau\\\\MemoiresStages\\\\Travaux_techniques\\\\Traitements\\\\Datasets\\\\trying_data_after_deduplication.csv'\n",
    "# else:\n",
    "#     output_path_after = f'/mnt/d/Bureau/MemoiresStages/Travaux_techniques/Traitements/Datasets/trying_data_after_deduplication.csv'\n",
    "\n",
    "# df.to_csv(output_path_after, index=False)\n",
    "\n",
    "# # Exporter les doublons supprimés\n",
    "# output_path_duplicates = r'D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets\\trying_data_duplicates.csv'\n",
    "# all_duplicates.to_csv(output_path_duplicates, index=False)\n",
    "\n",
    "# print(f\"Fichier après suppression des doublons sauvegardé sous {output_path_after}\")\n",
    "# print(f\"Doublons supprimés sauvegardés sous {output_path_duplicates}\")\n",
    "\n",
    "\n",
    "\n",
    "# Ajouter une colonne 'nb_pieces' en extrayant le nombre de pièces du titre\n",
    "def extract_nb_pieces(title):\n",
    "    match = re.search(r'\\b(\\d+)\\s*(pi[eè]ces?|pcs?)\\b', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    if \"studio\" in title or \"chambre salon\" in title:\n",
    "        return 1\n",
    "    return None\n",
    "\n",
    "if 'title' in df.columns:\n",
    "    df['nb_pieces'] = df['title'].apply(extract_nb_pieces)\n",
    "\n",
    "# Convertir 'nb_pieces' en int\n",
    "df['nb_pieces'] = df['nb_pieces'].astype('Int64')\n",
    "\n",
    "# Détection et suppression des lignes évoquant la vente, entrepôt ou avec un prix > 10000000\n",
    "vente_keywords = ['vente', 'vendre', 'à vendre', 'a vendre', 'prix de vente', 'coût de vente', 'cout de vente']\n",
    "entrepot_keywords = ['entrepôt', 'entrepot', 'hotel', 'hôtel', 'saisonnière', 'saisonniere']\n",
    "\n",
    "def is_for_sale_or_entrepot(description, price):\n",
    "    if isinstance(description, str) and (any(keyword in description for keyword in vente_keywords) or any(keyword in description for keyword in entrepot_keywords)):\n",
    "        return True\n",
    "    if price and price > 10000000:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "df = df[~df.apply(lambda row: is_for_sale_or_entrepot(row['description'], row['price']), axis=1)]\n",
    "\n",
    "\n",
    "# Supprimer les lignes contenant plusieurs biens immobiliers dans le titre\n",
    "multiple_properties_regex1 = r'appartements|studios|maisons|villas|bureaux'\n",
    "df = df[~df['title'].str.contains(multiple_properties_regex1, case=False, na=False)]\n",
    "\n",
    "# Supprimer les lignes contenant plusieurs biens immobiliers dans le titre\n",
    "multiple_properties_regex2 = r'appartements?[^,]*,|appartements?[^et]*et|maisons?,|bureaux?,|studios?,|villas?,'\n",
    "df = df[~df['title'].str.contains(multiple_properties_regex2, case=False, na=False)]\n",
    "\n",
    "# Supprimer les lignes contenant \"et\" dans le titre\n",
    "df = df[~df['title'].str.contains(r'\\bet\\b', case=False, na=False)]\n",
    "\n",
    "# Supprimer les lignes contenant \"terrain\" dans le titre\n",
    "df = df[~df['title'].str.contains(r'\\bterrain\\b', case=False, na=False)]\n",
    "\n",
    "\n",
    "def correct_price_from_description(description, price):\n",
    "    if not isinstance(description, str):\n",
    "        return price\n",
    "\n",
    "    # Suppression des lignes avec paiement journalier dans la description\n",
    "    if re.search(r'\\b(par jours?|/ jours?|parjours?|/jours?|/ 1 jours?|/1 jours?|la nuitée|la nuitee|la nuit|par nuit|jour)\\b', description):\n",
    "        return None\n",
    "\n",
    "    # List of regex patterns to match different price formats\n",
    "    patterns = [\n",
    "        r'\\b(loyer|prix|prix location|cout location|location|cout|coût)\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bà\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bloyer\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bcout\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bprix\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\b(loyer|prix)\\s*=\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\b(loyer|prix|location)\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa)?(?!.*frais)',\n",
    "        r'\\bà\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\b(loyer|prix)\\s*[:\\s]*([\\d\\s]+)\\s*(f|fcfa|francs|cfa|francs cfa|cfa)?',\n",
    "        r'\\b(loyer)\\s*:\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bloyer\\s*:\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bà\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bloyer\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bà\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\bloyer\\s*[:\\s]*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?',\n",
    "        r'\\b(loyer|prix|location)\\s*:\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa)?\\s*x?\\s*[\\d]*(?!.*frais)',\n",
    "        r'\\b(loyer|prix)\\s*[:\\s]*([\\d\\s]+)\\s*(f|fcfa|francs|cfa|francs cfa|cfa)?',\n",
    "        r'\\bloyer\\s*:\\s*([\\d\\s]+)\\s*(mille|frs|fcfa|f|francs|f cfa|cfa|fcfa/mois)?'\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, description)\n",
    "        if match:\n",
    "            price_str = re.sub(r'\\s+', '', match.group(2) if len(match.groups()) > 1 and match.group(2) else match.group(1))\n",
    "            if price_str.isdigit():\n",
    "                corrected_price = int(price_str)\n",
    "                if len(match.groups()) > 2 and match.group(3) and 'mille' in match.group(3):\n",
    "                    corrected_price *= 1000\n",
    "                return corrected_price\n",
    "    return price\n",
    "\n",
    "# Supprimer les lignes avec des descriptions manquantes ou 'None'\n",
    "df = df.dropna(subset=['description'])\n",
    "\n",
    "# Appliquer la correction des prix sur la colonne 'description' et supprimer les lignes contenant 'par jour'\n",
    "df['price'] = df.apply(lambda row: correct_price_from_description(row['description'], row['price']), axis=1)\n",
    "df = df.dropna(subset=['price'])\n",
    "\n",
    "# Supprimer les lignes avec un prix inférieur à 40000\n",
    "df = df[df['price'] > 39999]\n",
    "\n",
    "\n",
    "def supprimer_lignes_nb_pieces_manquant(df):\n",
    "    # Définir le motif regex\n",
    "    motif = re.compile(r'\\b(\\d+)\\s*(pi[eè]ces?|pcs?)\\b')\n",
    "\n",
    "    def garder_ligne(row):\n",
    "        # Vérifier si 'nb_pieces' est manquant\n",
    "        if pd.isna(row['nb_pieces']):\n",
    "            # Vérifier si le motif n'est pas trouvé dans la description\n",
    "            if not motif.search(str(row['description'])):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Appliquer la fonction pour filtrer les lignes\n",
    "    df = df[df.apply(garder_ligne, axis=1)]\n",
    "    return df\n",
    "\n",
    "# Appliquer la fonction au dataframe\n",
    "df = supprimer_lignes_nb_pieces_manquant(df)\n",
    "\n",
    "def imputer_nb_pieces(df):\n",
    "    # Définir le motif regex pour trouver le nombre de pièces\n",
    "    motif = re.compile(r'\\b(\\d+)\\s*(pi[eè]ces?|pcs?)\\b')\n",
    "\n",
    "    def imputer_ligne(row):\n",
    "        # Vérifier si 'nb_pieces' est manquant\n",
    "        if pd.isna(row['nb_pieces']):\n",
    "            # Rechercher le motif dans la description\n",
    "            match = motif.search(str(row['description']))\n",
    "            if match:\n",
    "                # Extraire et retourner le nombre de pièces\n",
    "                row['nb_pieces'] = int(match.group(1))\n",
    "        return row\n",
    "\n",
    "    # Appliquer la fonction pour imputer les valeurs manquantes de 'nb_pieces'\n",
    "    df = df.apply(imputer_ligne, axis=1)\n",
    "    return df\n",
    "\n",
    "# Appliquer la fonction au dataframe\n",
    "df = imputer_nb_pieces(df)\n",
    "\n",
    "# Séparer le DataFrame en deux, un pour \"ci.coinafrique.com\" et l'autre pour les autres sites\n",
    "df_coinafrique = df[df['site'] == 'ci.coinafrique.com']\n",
    "df_others = df[df['site'] != 'ci.coinafrique.com']\n",
    "\n",
    "# Traitement spécifique pour \"ci.coinafrique.com\"\n",
    "\n",
    "# Fonction pour nettoyer les colonnes en supprimant tous les caractères non numériques sauf le point décimal\n",
    "def clean_numeric_column(column):\n",
    "    return column.apply(lambda x: re.sub(r'[^\\d.]', '', str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Nettoyer les colonnes 'nb_pieces' et 'nb_salle_de_bain'\n",
    "df['nb_pieces'] = clean_numeric_column(df['nb_pieces'])\n",
    "df['nb_salle_de_bain'] = clean_numeric_column(df['nb_salle_de_bain'])\n",
    "\n",
    "# Convertir les colonnes 'nb_pieces' et 'nb_salle_de_bain' en int\n",
    "df['nb_pieces'] = pd.to_numeric(df_coinafrique['nb_pieces'], errors='coerce').astype('Int64')\n",
    "df_coinafrique['nb_salle_de_bain'] = pd.to_numeric(df_coinafrique['nb_salle_de_bain'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "# Fonction pour extraire la localisation à partir du titre\n",
    "def extract_localisation(title):\n",
    "    match = re.search(r'à\\s(.+)$', title)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r'-\\s(.+)$', title)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "# Fonction pour extraire le type de logement à partir du titre\n",
    "def extract_logement_type(title):\n",
    "    logement_types = ['appartement', 'maison', 'studio', 'villa', 'chambre', 'bureau', 'penthouse']\n",
    "    for logement in logement_types:\n",
    "        if re.search(r'\\b' + logement + r'\\b', title, re.IGNORECASE):\n",
    "            return logement\n",
    "    return None\n",
    "\n",
    "# Appliquer les fonctions pour extraire la localisation et le type de logement\n",
    "df_coinafrique['localisation'] = df_coinafrique['title'].apply(extract_localisation)\n",
    "df_coinafrique[\"type d'immobilier\"] = df_coinafrique['title'].apply(extract_logement_type)\n",
    "\n",
    "\n",
    "# Fusionner les deux DataFrames\n",
    "df = pd.concat([df_coinafrique, df_others], ignore_index=True)\n",
    "\n",
    "# Supprimer les lignes qui ont None dans la variable localisation\n",
    "df = df.dropna(subset=['localisation'])\n",
    "\n",
    "# Fonction pour déterminer le type d'immobilier\n",
    "def classify_immobilier(row):\n",
    "    text = ' '.join([str(row['title']), str(row['description']), str(row[\"type d'immobilier\"])])\n",
    "    if re.search(r'duplex', text, re.IGNORECASE):\n",
    "        return 'maison duplex'\n",
    "    if re.search(r'appartement|penthouse', text, re.IGNORECASE):\n",
    "        return 'appartement moderne'\n",
    "    if re.search(r'maison', text, re.IGNORECASE):\n",
    "        return 'maison'\n",
    "    if re.search(r'villa', text, re.IGNORECASE):\n",
    "        return 'villa moderne'\n",
    "    if re.search(r'studio', text, re.IGNORECASE):\n",
    "        return 'studio'\n",
    "    return 'autre'\n",
    "\n",
    "# Appliquer la fonction pour créer la colonne 'type_d_immobilier'\n",
    "df['type_d_immobilier'] = df.apply(classify_immobilier, axis=1)\n",
    "\n",
    "\n",
    "# Fonction pour déterminer si les toilettes sont internes\n",
    "def has_toilettes_internes(description):\n",
    "    return 0 if re.search(r\"(toilette|wc|douche|salle d'eau) externe\", description, re.IGNORECASE) else 1\n",
    "\n",
    "# Fonction pour déterminer si la cour est commune\n",
    "def has_cours_commune(description):\n",
    "    return 1 if re.search(r\"cours commune\", description, re.IGNORECASE) else 0\n",
    "\n",
    "# Appliquer les fonctions pour créer les nouvelles variables\n",
    "# df['Avec_eau'] = 1\n",
    "# df['Avec_electricite'] = 1\n",
    "# df['Materiaux_en_dur'] = 1\n",
    "df['Toilettes_internes'] = df['description'].apply(has_toilettes_internes)\n",
    "df['cours_commune'] = df['description'].apply(has_cours_commune)\n",
    "# df['habitee_par_un_seul_locataire'] = 1\n",
    "\n",
    "\n",
    "# Définir la liste des quartiers chics\n",
    "quartiers_chics = ['cocody', 'riviera', 'le plateau', 'zone 4', r'angr(é|e)', 'vallons', 'ambassade', 'golf']\n",
    "\n",
    "# Fonction pour déterminer si la localisation est dans un quartier chic\n",
    "def is_quartier_chic(localisation):\n",
    "    for quartier in quartiers_chics:\n",
    "        if re.search(quartier, localisation, re.IGNORECASE):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Appliquer la fonction pour créer la variable 'quartier_chic'\n",
    "df['quartier_chic'] = df['localisation'].apply(is_quartier_chic)\n",
    "\n",
    "# Créer le dictionnaire de mappage\n",
    "code_mapping = {\n",
    "    ('maison duplex', 1, 0, 1): \"401000006\",\n",
    "    ('maison duplex', 1, 0, 0): \"401000006\",\n",
    "    ('appartement moderne', 1, 0, 1): \"401000002\",\n",
    "    ('appartement moderne', 1, 0, 0): \"401000002\",\n",
    "    ('maison',  1, 0, 1): \"401000005\",\n",
    "    ('maison',  1, 0, 0): \"401000005\",\n",
    "    ('villa moderne', 1, 0, 1): \"401000010\",\n",
    "    ('villa moderne', 1, 0, 0): \"401000011\",\n",
    "    ('studio',  1, 0, 1): \"401000008\",\n",
    "    ('studio',  1, 0, 0): \"401000008\",\n",
    "    ('studio', 0, 1, 0): \"401000009\"\n",
    "}\n",
    "\n",
    "# Fonction pour obtenir le code de classification\n",
    "def get_classification_code(row, code_mapping):\n",
    "    key = (\n",
    "        row['type_d_immobilier'],\n",
    "        # row['Avec_eau'],\n",
    "        # row['Avec_electricite'],\n",
    "        # row['Materiaux_en_dur'],\n",
    "        row['Toilettes_internes'],\n",
    "        row['cours_commune'],\n",
    "        # row['habitee_par_un_seul_locataire'],\n",
    "        row['quartier_chic']\n",
    "    )\n",
    "    code = code_mapping.get(key, None)\n",
    "    if code is None:\n",
    "        print(f\"Key not found: {key}\")\n",
    "    return code\n",
    "\n",
    "# Appliquer la fonction pour créer la colonne 'code_var 2014'\n",
    "df['code_var 2014'] = df.apply(get_classification_code, axis=1, code_mapping=code_mapping)\n",
    "\n",
    "df.dropna(subset='code_var 2014')\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Définir les dictionnaires de mots regroupés\n",
    "commodities = {\n",
    "    'wifi': [r'wifi', r'internet', r'connexion', r'fibre optique'],\n",
    "    'jardin': [r'jardin(\\.locat\\d+)?'],\n",
    "    'meubler': [r'meubl(er|é|e)'],\n",
    "    'climatisation': [r'climat(isé|isation|iseur)', r'air conditionner', r'split'],\n",
    "    'garage': [r'garage'],\n",
    "    'balcon': [r'balcon'],\n",
    "    'sécuriser': [r'sécur(is|it|is)(er|e|é)'],\n",
    "    'étage': [r'étage'],\n",
    "    'parking': [r'parking'],\n",
    "    'douche': [r'douche'],\n",
    "    'séjour': [r'séjour', r'sejour'],\n",
    "    'équiper': [r'équip(er|é|e|ée|ement)'],\n",
    "    'placard': [r'placard'],\n",
    "    'salle': [r'salle'],\n",
    "    'manger': [r'manger'],\n",
    "    'bureau': [r'bureau'],\n",
    "    'piscine': [r'piscine'],\n",
    "    'gardien': [r'gardi(en|en)', r'garde'],\n",
    "    'électrogène': [r'électrogène'],\n",
    "    'four': [r'four'],\n",
    "    'cinéma': [r'cinéma', r'netflix'],\n",
    "    'ascenseur': [r'ascenseur'],\n",
    "    'double vitrage': [r'double vitrage'],\n",
    "    'terrasse': [r'terrasse'],\n",
    "    'toilette': [r'toilette', r'wc'],\n",
    "    'buanderie': [r'buanderie'],\n",
    "    'chauffe': [r'chauff(e|e-eau)'],\n",
    "    'cuisine': [r'cuisine']\n",
    "}\n",
    "\n",
    "natural_externalities = {\n",
    "    'bord_de_mer': [r'plage', r'mer', r'océan'],\n",
    "    'forêt': [r'for(ê|e)t'],\n",
    "    'parc': [r'parc'],\n",
    "    'montagne': [r'montagne'],\n",
    "    'rivière': [r'rivi(è|e)re'],\n",
    "    'lac': [r'lac']\n",
    "}\n",
    "\n",
    "artificial_externalities = {\n",
    "    'zone_commerciale': [r'centre commercial', r'marché', r'zone commerciale'],\n",
    "    'pharmacie': [r'pharmacie'],\n",
    "    'restaurant': [r'restaurant', r'resto'],\n",
    "    'bar': [r'bar', r'maquis'],\n",
    "    'ambassade': [r'ambassade'],\n",
    "    'école': [r'écol(e|es)', r'crèch(e|es)', r'lycé(e|es)', r'universit(é|és)'],\n",
    "    'hôpital': [r'hôpital', r'clinique', r'dispensaire', r'CHU', r'CHR', r'medecin']\n",
    "}\n",
    "\n",
    "# Fonction pour extraire les informations de la description\n",
    "def extract_description_info(df, grouped_words):\n",
    "    def extract_info(description):\n",
    "        if not isinstance(description, str):\n",
    "            return {key: 0 for key in grouped_words}\n",
    "        \n",
    "        doc = nlp(description)\n",
    "        info = {key: 0 for key in grouped_words}\n",
    "        \n",
    "        negation = False\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'neg':\n",
    "                negation = True\n",
    "            for key, patterns in grouped_words.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, token.lemma_):\n",
    "                        info[key] = 0 if negation else 1\n",
    "                        negation = False\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    description_info = df['description'].apply(extract_info)\n",
    "    \n",
    "    for key in grouped_words.keys():\n",
    "        df[key] = description_info.apply(lambda x: x[key])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer les fonctions pour extraire les informations des commodités et externalités\n",
    "df = extract_description_info(df, commodities)\n",
    "df = extract_description_info(df, natural_externalities)\n",
    "df = extract_description_info(df, artificial_externalities)\n",
    "\n",
    "# Créer les variables 'commodités', 'externalités_naturelles' et 'externalités_artificielles'\n",
    "df['commodités'] = df[list(commodities.keys())].max(axis=1)\n",
    "df['externalités_naturelles'] = df[list(natural_externalities.keys())].max(axis=1)\n",
    "df['externalités_artificielles'] = df[list(artificial_externalities.keys())].max(axis=1)\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Exporter le dataframe final avec les modifications\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    output_path_final = f'D:\\\\Bureau\\\\MemoiresStages\\\\Travaux_techniques\\\\Traitements\\\\Datasets\\\\trying_data_final.csv'\n",
    "else:\n",
    "    output_path_final = f'/mnt/d/Bureau/MemoiresStages/Travaux_techniques/Traitements/Datasets/trying_data_final.csv'\n",
    "\n",
    "df.to_csv(output_path_final, index=False)\n",
    "\n",
    "print(f\"Fichier final avec les modifications sauvegardé sous {output_path_final}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
