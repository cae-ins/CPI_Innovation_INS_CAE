{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les traitements sont en cours pour le mois de Mai, cela pourrait prendre quelques minutes...\n",
      "Restants : 3998\n",
      "2.2.\tConvertir toutes les données en minuscules pour faciliter le traitement des données textuelles\n",
      "Restants : 3966\n",
      "2.3. Création des variables « site » et « date »\n",
      "2.4.\tSuppression des lignes non pertinentes pour l’étude (vente, entrepôt, terrains, etc.)\n",
      "Restants : 797\n",
      "2.5.\tNumérisation des variables quantitatives (nombre de pièces, nombre de salle de bain, loyer_mensuel et superficie en m2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:116: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['concat_description_title'].str.contains(daily_and_no_location, case=False, na=False)]\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:125: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['title'].str.contains(multiple_properties_pattern, case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restants : 484\n",
      "2.6.\tSuppression des doublons\n",
      "Nombre de doublons primaires détectés: 206\n",
      "Nombre de doublons secondaires détectés: 18\n",
      "Restants 292\n",
      "2.7.\tCréation des granularités de la localisation : nouvelles variables de localisation\n",
      "Restants : 292\n",
      "2.8.\tCorrection du type d’immobilier à partir de la variable « description » \n",
      "   type d'immobilier type_immobilier\n",
      "0       appartements          duplex\n",
      "2       appartements          maison\n",
      "3       appartements          maison\n",
      "8       appartements     appartement\n",
      "11      appartements          maison\n",
      "Restants : 292\n",
      " 2.9.\tCréation de variables de classification des variétés (eau, électricité, type de matériaux, toilette interne ou externe, cours commune, quartiers_chics, nb_pieces_classes)\n",
      "Restants : 292\n",
      "Fichier final avec les modifications sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets_pretraites\\pretraitees\\Mai_pretraitee.xlsx\n",
      "Les traitements sont en cours pour le mois de Juin, cela pourrait prendre quelques minutes...\n",
      "Restants : 16011\n",
      "2.2.\tConvertir toutes les données en minuscules pour faciliter le traitement des données textuelles\n",
      "Restants : 15901\n",
      "2.3. Création des variables « site » et « date »\n",
      "2.4.\tSuppression des lignes non pertinentes pour l’étude (vente, entrepôt, terrains, etc.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:116: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['concat_description_title'].str.contains(daily_and_no_location, case=False, na=False)]\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:125: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['title'].str.contains(multiple_properties_pattern, case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restants : 2408\n",
      "2.5.\tNumérisation des variables quantitatives (nombre de pièces, nombre de salle de bain, loyer_mensuel et superficie en m2)\n",
      "Restants : 1679\n",
      "2.6.\tSuppression des doublons\n",
      "Nombre de doublons primaires détectés: 1159\n",
      "Nombre de doublons secondaires détectés: 59\n",
      "Restants 656\n",
      "2.7.\tCréation des granularités de la localisation : nouvelles variables de localisation\n",
      "Restants : 656\n",
      "2.8.\tCorrection du type d’immobilier à partir de la variable « description » \n",
      "   type d'immobilier type_immobilier\n",
      "0       appartements          maison\n",
      "1       appartements          maison\n",
      "6       appartements     appartement\n",
      "9       appartements          maison\n",
      "11      appartements          maison\n",
      "Restants : 656\n",
      " 2.9.\tCréation de variables de classification des variétés (eau, électricité, type de matériaux, toilette interne ou externe, cours commune, quartiers_chics, nb_pieces_classes)\n",
      "Restants : 656\n",
      "Fichier final avec les modifications sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets_pretraites\\pretraitees\\Juin_pretraitee.xlsx\n",
      "Les traitements sont en cours pour le mois de Juillet, cela pourrait prendre quelques minutes...\n",
      "Restants : 13446\n",
      "2.2.\tConvertir toutes les données en minuscules pour faciliter le traitement des données textuelles\n",
      "Restants : 13208\n",
      "2.3. Création des variables « site » et « date »\n",
      "2.4.\tSuppression des lignes non pertinentes pour l’étude (vente, entrepôt, terrains, etc.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:116: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['concat_description_title'].str.contains(daily_and_no_location, case=False, na=False)]\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:125: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['title'].str.contains(multiple_properties_pattern, case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restants : 5040\n",
      "2.5.\tNumérisation des variables quantitatives (nombre de pièces, nombre de salle de bain, loyer_mensuel et superficie en m2)\n",
      "Restants : 3251\n",
      "2.6.\tSuppression des doublons\n",
      "Nombre de doublons primaires détectés: 1200\n",
      "Nombre de doublons secondaires détectés: 209\n",
      "Restants 2298\n",
      "2.7.\tCréation des granularités de la localisation : nouvelles variables de localisation\n",
      "Restants : 2298\n",
      "2.8.\tCorrection du type d’immobilier à partir de la variable « description » \n",
      "   type d'immobilier type_immobilier\n",
      "56       appartement          maison\n",
      "66       appartement     appartement\n",
      "72       appartement     appartement\n",
      "74       appartement     appartement\n",
      "76       appartement          duplex\n",
      "Restants : 2298\n",
      " 2.9.\tCréation de variables de classification des variétés (eau, électricité, type de matériaux, toilette interne ou externe, cours commune, quartiers_chics, nb_pieces_classes)\n",
      "Restants : 2298\n",
      "Fichier final avec les modifications sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets_pretraites\\pretraitees\\Juillet_pretraitee.xlsx\n",
      "Les traitements sont en cours pour le mois de Aout, cela pourrait prendre quelques minutes...\n",
      "Restants : 8039\n",
      "2.2.\tConvertir toutes les données en minuscules pour faciliter le traitement des données textuelles\n",
      "Restants : 7614\n",
      "2.3. Création des variables « site » et « date »\n",
      "2.4.\tSuppression des lignes non pertinentes pour l’étude (vente, entrepôt, terrains, etc.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:116: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['concat_description_title'].str.contains(daily_and_no_location, case=False, na=False)]\n",
      "C:\\Users\\Wilfried KOMENAN\\AppData\\Local\\Temp\\ipykernel_102948\\3559000626.py:125: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['title'].str.contains(multiple_properties_pattern, case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restants : 4541\n",
      "2.5.\tNumérisation des variables quantitatives (nombre de pièces, nombre de salle de bain, loyer_mensuel et superficie en m2)\n",
      "Restants : 3022\n",
      "2.6.\tSuppression des doublons\n",
      "Nombre de doublons primaires détectés: 2814\n",
      "Nombre de doublons secondaires détectés: 66\n",
      "Restants 812\n",
      "2.7.\tCréation des granularités de la localisation : nouvelles variables de localisation\n",
      "Restants : 812\n",
      "2.8.\tCorrection du type d’immobilier à partir de la variable « description » \n",
      "    type d'immobilier type_immobilier\n",
      "25            maisons           villa\n",
      "43            maisons          studio\n",
      "44            maisons           villa\n",
      "66            maisons          maison\n",
      "125           maisons     appartement\n",
      "Restants : 812\n",
      " 2.9.\tCréation de variables de classification des variétés (eau, électricité, type de matériaux, toilette interne ou externe, cours commune, quartiers_chics, nb_pieces_classes)\n",
      "Restants : 812\n",
      "Fichier final avec les modifications sauvegardé sous D:\\Bureau\\MemoiresStages\\Travaux_techniques\\Traitements\\Datasets_pretraites\\pretraitees\\Aout_pretraitee.xlsx\n"
     ]
    }
   ],
   "source": [
    "# !pip install dico/fr_core_news_md-3.7.0-py3-none-any.whl\n",
    "# !pip install pandas spacy matplotlib\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install regex\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from fuzzywuzzy import fuzz\n",
    "import platform\n",
    "\n",
    "# Charger le modèle spaCy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "\n",
    "list_mois = [\n",
    "    \"Mai\", \n",
    "    \"Juin\", \n",
    "    \"Juillet\",\n",
    "    \"Aout\"\n",
    "    ]\n",
    "\n",
    "for mois in list_mois:\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        folder = f'D:\\\\Bureau\\\\MemoiresStages\\\\Travaux_techniques\\\\Scrapping\\\\Datasets\\\\{mois}'\n",
    "    else:\n",
    "        folder = f'/mnt/d/Bureau/MemoiresStages/Travaux_techniques/Scrapping/Datasets/{mois}'\n",
    "\n",
    "\n",
    "    # Fonction pour obtenir tous les fichiers CSV dans un dossier et ses sous-dossiers\n",
    "    def get_all_csv_files(folder):\n",
    "        csv_files = []\n",
    "        for root, _, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_files.append(os.path.join(root, file))\n",
    "        print(f\"Les traitements sont en cours pour le mois de {mois}, cela pourrait prendre quelques minutes...\")\n",
    "        return csv_files\n",
    "\n",
    "    file_paths = get_all_csv_files(folder)\n",
    "\n",
    "    # Charger et fusionner les fichiers\n",
    "    df_list = []\n",
    "    for file in file_paths:\n",
    "        try:\n",
    "            # Vérifier si le fichier est vide avant de lire\n",
    "            if os.path.getsize(file) > 0:\n",
    "                df = pd.read_csv(file)\n",
    "                if not df.empty:\n",
    "                    df_list.append(df)\n",
    "                else:\n",
    "                    print(f\"Le fichier {file} est vide et a été ignoré.\")\n",
    "            else:\n",
    "                print(f\"Le fichier {file} est vide et a été ignoré.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Le fichier {file} est mal formaté et a été ignoré.\")\n",
    "\n",
    "    # Fusionner les DataFrames non vides\n",
    "    if df_list:\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"Restants : {len(df)}\")\n",
    "    else:\n",
    "        print(\"Aucun fichier valide n'a été trouvé.\")\n",
    "\n",
    "\n",
    "    print(\"2.2.\tConvertir toutes les données en minuscules pour faciliter le traitement des données textuelles\")\n",
    "\n",
    "    # Convertir toutes les valeurs en minuscules pour les colonnes de type str, sauf pour les colonnes spécifiées\n",
    "    columns_to_exclude = ['superficie', 'nb_pieces', 'nb_salle_de_bain', 'scraping_date', 'link']\n",
    "    df = df.apply(lambda col: col.str.lower() if col.name not in columns_to_exclude and col.dtype == 'object' else col)\n",
    "\n",
    "    # garder les lignes dont les descriptions contiennent plus de 30 mots\n",
    "    seuil_nombre_tokens = 15\n",
    "    df = df[df['description'].str.split().str.len() >= seuil_nombre_tokens]\n",
    "\n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "\n",
    "    print(\"2.3. Création des variables « site » et « date »\")\n",
    "\n",
    "    # Fonction pour extraire le nom de domaine à partir de l'URL\n",
    "    def extract_site(link):\n",
    "        try:\n",
    "            return urlparse(link).netloc\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Ajouter la colonne 'site' en extrayant le nom de domaine de la colonne 'link'\n",
    "    if 'link' in df.columns:\n",
    "        df['site'] = df['link'].apply(extract_site)\n",
    "        \n",
    "\n",
    "    if 'scraping_date' in df.columns:\n",
    "        df = df.drop(columns=['scraping_date'])\n",
    "    else:\n",
    "        print(\"La colonne 'scraping_date' n'existe pas dans le DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"2.4.\tSuppression des lignes non pertinentes pour l’étude (vente, entrepôt, terrains, etc.)\")\n",
    "\n",
    "    # Supprimer les lignes où 'price' contient des indications de loyer journalier\n",
    "    daily_and_no_location = r'\\b(jour|vente|vendre|nuit|entrepôt|hôtel|entrepot|hotel|terrain|triplex|résidence|residence|residentiel|résidentiel|golf|ambassde|zone4|zone 4|haut stanting|luxueux|luxueuse|luxueuses|résidences|residences|residentiels|résidentiels)\\b'\n",
    "\n",
    "    # Créer une nouvelle colonne temporaire qui concatène 'description' et 'title'\n",
    "    df['concat_description_title'] = df['description'].fillna('') + ' ' + df['title'].fillna('') + ' ' + df['price'].fillna('') + ' ' + df['localisation'].fillna('')\n",
    "\n",
    "    # Filtrer les lignes qui ne contiennent pas les mots-clés\n",
    "    df = df[~df['concat_description_title'].str.contains(daily_and_no_location, case=False, na=False)]\n",
    "\n",
    "    # Supprimer la colonne temporaire après le filtrage\n",
    "    df = df.drop(columns=['concat_description_title'])\n",
    "\n",
    "    # Supprimer les lignes où title est compose de plusieurs logements\n",
    "    multiple_properties_pattern = r'\\b(et|appartements|studios|maisons|villas|bureaux|penthouses)\\b'\n",
    "\n",
    "    # Filtrer les lignes qui ne contiennent pas les mots-clés\n",
    "    df = df[~df['title'].str.contains(multiple_properties_pattern, case=False, na=False)]\n",
    "\n",
    "\n",
    "    # Afficher le nombre de lignes restantes après le filtrage\n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "    print(\"2.5.\tNumérisation des variables quantitatives (nombre de pièces, nombre de salle de bain, loyer_mensuel et superficie en m2)\")\n",
    "\n",
    "    # Fonction pour nettoyer et convertir les valeurs de 'price' en format numérique\n",
    "    def clean_price(price):\n",
    "        # Extraire les chiffres de la chaîne de caractères\n",
    "        price = re.sub(r'[^\\d]', '', str(price))\n",
    "        # Convertir en entier\n",
    "        return float(price) if price.isdigit() else None\n",
    "    # Appliquer la fonction de nettoyage sur la colonne 'price', et traduction du nom de la variable\n",
    "    if 'price' in df.columns:\n",
    "        df['loyer_mensuel'] = df['price'].apply(clean_price)\n",
    "        df.drop(columns=['price'], inplace=True)\n",
    "        \n",
    "    # Fonction pour nettoyer et convertir les valeurs de 'superficie' en format float\n",
    "    def clean_superficie(superficie):\n",
    "        # Retirer toutes les occurrences de \"m2\" et extraire les chiffres\n",
    "        superficie = re.sub(r'\\s*m2\\s*', '', str(superficie), flags=re.IGNORECASE)\n",
    "        # Convertir en float\n",
    "        try:\n",
    "            return float(superficie)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    # Appliquer la fonction de nettoyage sur la colonne 'superficie' et modification du nom de la colonne\n",
    "    if 'superficie' in df.columns:\n",
    "        df['superficie_m2'] = df['superficie'].apply(clean_superficie)\n",
    "        df.drop(columns=['superficie'], inplace=True)\n",
    "        \n",
    "    # Fonction pour nettoyer la colonne 'nb_salle_de_bain'\n",
    "    def clean_nb_salle_de_bain(nb_salle_de_bain):\n",
    "        if pd.isna(nb_salle_de_bain):\n",
    "            return None\n",
    "        # Utiliser une expression régulière pour extraire le nombre\n",
    "        match = re.search(r'\\d+', nb_salle_de_bain)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return None\n",
    "    # Convertir toute la colonne 'nb_salle_de_bain' en string avant de faire le traitement\n",
    "    if 'nb_salle_de_bain' in df.columns:\n",
    "        df['nb_salle_de_bain'] = df['nb_salle_de_bain'].astype(str)\n",
    "        df['nb_salle_de_bain'] = df['nb_salle_de_bain'].apply(clean_nb_salle_de_bain)\n",
    "    # Convertir la colonne 'nb_salle_de_bain' en type Int64\n",
    "    df['nb_salle_de_bain'] = df['nb_salle_de_bain'].astype('Int64')\n",
    "\n",
    "\n",
    "    # Fonction pour nettoyer la colonne 'nb_pieces' et la convertir en int\n",
    "    def clean_nb_pieces(nb_pieces):\n",
    "        if pd.isna(nb_pieces):\n",
    "            return None\n",
    "        # Extraire les chiffres de la chaîne de caractères et les convertir en entier\n",
    "        match = re.search(r'\\d+', str(nb_pieces))\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return None\n",
    "\n",
    "    # Appliquer la fonction pour nettoyer 'nb_pieces'\n",
    "    df['nb_pieces'] = df['nb_pieces'].apply(clean_nb_pieces)\n",
    "\n",
    "\n",
    "    def extract_nb_pieces_if_different(row):\n",
    "        # Vérifier si 'title' est bien une chaîne de caractères\n",
    "        if isinstance(row['title'], str):\n",
    "            # Extraire le nombre de pièces du titre\n",
    "            match = re.search(r'\\b(\\d+)\\s*(pi[eè]ces?|pcs?)\\b', row['title'])\n",
    "            if match:\n",
    "                nb_pieces_title = int(match.group(1))\n",
    "                # Vérifier si le nombre de pièces extrait du titre est différent de celui existant\n",
    "                if row['nb_pieces'] != nb_pieces_title:\n",
    "                    return nb_pieces_title\n",
    "            # Cas particulier pour \"studio\" ou \"chambre salon\"\n",
    "            if \"studio\" in row['title'].lower() or \"chambre salon\" in row['title'].lower():\n",
    "                return 1\n",
    "        return row['nb_pieces']\n",
    "\n",
    "    # Appliquer la fonction sur chaque ligne du DataFrame\n",
    "    if 'title' in df.columns and 'nb_pieces' in df.columns:\n",
    "        df['nb_pieces'] = df.apply(extract_nb_pieces_if_different, axis=1)\n",
    "\n",
    "    # Convertir 'nb_pieces' en int (si cela est nécessaire)\n",
    "    df['nb_pieces'] = df['nb_pieces'].astype('Int64')\n",
    "\n",
    "\n",
    "    def imputer_nb_pieces(df):\n",
    "        # Définir le motif regex pour trouver le nombre de pièces\n",
    "        motif = re.compile(r'\\b(\\d+)\\s*(pi[eè]ces?|pcs?)\\b')\n",
    "\n",
    "        def imputer_ligne(row):\n",
    "            # Vérifier si 'nb_pieces' est manquant\n",
    "            if pd.isna(row['nb_pieces']):\n",
    "                # Rechercher le motif dans la description\n",
    "                match = motif.search(str(row['description']))\n",
    "                if match:\n",
    "                    # Extraire et retourner le nombre de pièces\n",
    "                    row['nb_pieces'] = int(match.group(1))\n",
    "            return row\n",
    "\n",
    "        # Appliquer la fonction pour imputer les valeurs manquantes de 'nb_pieces'\n",
    "        df = df.apply(imputer_ligne, axis=1)\n",
    "        return df\n",
    "\n",
    "    # Appliquer la fonction au dataframe\n",
    "    df = imputer_nb_pieces(df)\n",
    "\n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.dropna(subset=['nb_pieces'])\n",
    "    \n",
    "    df = df[(df['loyer_mensuel'] <= 600000) & (df['loyer_mensuel'] >= 20000)]\n",
    "\n",
    "    \n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "\n",
    "    print(\"2.6.\tSuppression des doublons\")\n",
    "    # Fonction pour gérer les doublons primaires\n",
    "    def remove_primary_duplicates(df):\n",
    "        primary_columns = [col for col in df.columns if col not in ['scraping_date', 'date', 'link']]\n",
    "        duplicates = df[df.duplicated(subset=primary_columns, keep=False)]\n",
    "        primary_duplicates_count = duplicates.shape[0]\n",
    "        print(f\"Nombre de doublons primaires détectés: {primary_duplicates_count}\")\n",
    "        df = df.drop_duplicates(subset=primary_columns, keep='first')\n",
    "        return df, duplicates\n",
    "\n",
    "    # Appliquer la suppression des doublons primaires\n",
    "    df, primary_duplicates = remove_primary_duplicates(df)\n",
    "\n",
    "\n",
    "    # Fonction pour gérer les doublons secondaires\n",
    "    def remove_secondary_duplicates(df):\n",
    "        df['description_clean'] = df['description'].apply(lambda x: re.sub(r'\\W+', ' ', str(x)))\n",
    "        duplicates = set()\n",
    "        seen = set()\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            if i in seen:\n",
    "                continue\n",
    "            for j, other_row in df.iterrows():\n",
    "                if i != j and j not in seen:\n",
    "                    similarity = fuzz.token_set_ratio(row['description_clean'], other_row['description_clean'])\n",
    "                    if similarity > 99.5:\n",
    "                        duplicates.add(j)\n",
    "                        seen.add(j)\n",
    "                        break  # Exit the inner loop to ensure only one duplicate is removed\n",
    "\n",
    "        secondary_duplicates_count = len(duplicates)\n",
    "        print(f\"Nombre de doublons secondaires détectés: {secondary_duplicates_count}\")\n",
    "\n",
    "        duplicates_list = list(duplicates)\n",
    "        secondary_duplicates = df.loc[duplicates_list]\n",
    "        df = df.drop(duplicates_list)\n",
    "        df.drop(columns=['description_clean'], inplace=True)\n",
    "        return df, secondary_duplicates\n",
    "\n",
    "    # Appliquer la suppression des doublons secondaires\n",
    "    df, secondary_duplicates = remove_secondary_duplicates(df)\n",
    "\n",
    "    print(f'Restants {len(df)}')\n",
    "\n",
    "\n",
    "    print(\"2.7.\tCréation des granularités de la localisation : nouvelles variables de localisation\")\n",
    "\n",
    "    # Dictionnaire pour les grappes et leurs régions associées\n",
    "    grappe_to_region = {\n",
    "        'abidjan': ['abidjan'],\n",
    "        'nord': [r'bafing', r'bagoué', r'folon', r'kabadougou', r'poro', r'tchologo', r'worodougou'],\n",
    "        'centre': [r'bélier', r'gbêkêe', r'hambol', r'iffou', r'marahoué', r'moronou', r'n’zi', r'yamoussoukro'],\n",
    "        'est': [r'bounkani', r'gontougo', r'indénié-djuablin', r'mé', r'sud-comoé', r'zanzan'],\n",
    "        'ouest': [r'agnéby-tiassa', r'béré', r'cavally', r'gbôklé', r'gôh', r'grands-ponts', r'guémon', r'haut-sassandra', r'lôh-djiboua', r'nawa', r'san-pédro', r'tonkpi']\n",
    "    }\n",
    "\n",
    "\n",
    "    region_to_ville = {\n",
    "        # Région d'Abidjan\n",
    "        'abidjan': ['abidjan'],\n",
    "        # Autres régions\n",
    "        'bafing': ['touba'],\n",
    "        'bagoué': [r'boundiali'],\n",
    "        'folon': ['minignan'],\n",
    "        'kabadougou': [r'odienn(é|e)'],\n",
    "        'poro': ['korhogo'],\n",
    "        'tchologo': [r'ferkess(é|e)dougou'],\n",
    "        'worodougou': [r's(é|e)gu(é|e)la'],\n",
    "        'bélier': [r'yamoussoukro'],\n",
    "        'gbêkê': [r'bouak(é|e)'],\n",
    "        'hambol': [r'katiola'],\n",
    "        'iffou': [r'daoukro'],\n",
    "        'marahoué': [r'bouafl(é|e)'],\n",
    "        'moronou': [r'bongouanou'],\n",
    "        'n’zi': [r'dimbokro'],\n",
    "        'bounkani': [r'bouna'],\n",
    "        'gontougo': [r'bondoukou'],\n",
    "        'indénié-djuablin': [r'abengourou'],\n",
    "        'mé': [r'adzop(é|e)'],\n",
    "        'sud-comoé': [r'(bassam|aboisso)'],\n",
    "        'agnéby-tiassa': [r'agboville'],\n",
    "        'béré': [r'mankono'],\n",
    "        'cavally': [r'guiglo'],\n",
    "        'gbôklé': [r'sassandra'],\n",
    "        'gôh': [r'gagnoa'],\n",
    "        'grands-ponts': [r'dabou'],\n",
    "        'guémon': [r'du(é|e)kou(é|e)'],\n",
    "        'haut-sassandra': [r'daloa'],\n",
    "        'lôh-djiboua': [r'divo'],\n",
    "        'nawa': [r'soubr(é|e)'],\n",
    "        'san-pédro': [r'san-p(é|e)dro'],\n",
    "        'tonkpi': [r'man']\n",
    "    }\n",
    "\n",
    "    # Dictionnaire pour les villes et leurs communes (seulement pour Abidjan)\n",
    "    ville_to_commune = {\n",
    "        'abidjan': ['abobo', 'adjamé', 'attécoubé', 'cocody', 'koumassi', 'marcory', 'plateau', 'port-bouët', 'treichville', 'yopougon', 'anyama', 'bingerville', 'songon']\n",
    "    }\n",
    "\n",
    "    # Dictionnaire pour les communes et leurs quartiers associés (seulement pour Abidjan)\n",
    "    commune_to_quartier = {\n",
    "        # Abobo\n",
    "        'abobo': [r'sogephia', r'avocatier', r'abobo doum(é|e)', r'pk18', r'anonkoua kout(é|e)', r'avocatier', r'belleville', r'derri(è|e)re rails', r'n’dotr(é|e)', r'samak(é|e)', r'anador'],\n",
    "        # Adjamé\n",
    "        'adjamé': [r'williamsville', r'bracodi', r'ind(é|e)ni(é|e)', r'220 logements', r'adjam(é|e) village', r'libert(é|e)', r'habitat', r'fraternit(é|e)'],\n",
    "        # Attécoubé\n",
    "        'attécoubé': [r'banco', r'abobodoum(é|e)', r'locodjro', r'mossikro', r'agban', r'anonkoua', r'att(é|e)coub(é|e) village'],\n",
    "        # Cocody\n",
    "        'cocody': [r'cnps', r'rti', r'cocody centre', r'9eme tranche', r'danga', r'angré', r'angre', r'abatta', r'riviera', r'riviéra', r'rivi(é|e)ra 1', r'rivi(é|e)ra 2', r'rivi(é|e)ra 3', r'rivi(é|e)ra 4', r'riviera4',r'palmeraie', r'deux plateaux', r'2 plateaux', r'ii plateaux', r'danga', r'mermoz', r'vallon', r'cité des arts', r'cité rouge', r'ambassade', r'golf', r'faya', 'bonoumin', 'attoban', 'lycée technique', 'riviera triangle', 'abatta', 'école de police', 'ecole de police', 'm\\'badon', 'riviera4'],\n",
    "        # Koumassi\n",
    "        'koumassi': [r'divo', r'remblais', r'prodomo', r'campement', r'sicogi', r'grand campement', r'zone industrielle', r'addoha'],\n",
    "        # Marcory\n",
    "        'marcory': [r'marcory résidentiel', r'orca deco', r'zone 4', r'bietry', r'biétry', r'anoumabo', r'hibiscus', r'r(é|e)sidentiel', r'zone 3', r'zone 4c', r'champroux', r'zone4'],\n",
    "        # Le Plateau\n",
    "        'plateau': [r'plateau ind(é|e)ni(é|e)', r'plateau dokui', r'plateau vallon', r'plateau centre', r'plateau nord', r'plateau sud', r'plateau',  r'zone ccia'],\n",
    "        # Port-Bouët\n",
    "        'port-bouët': [r'anani', r'gonzagueville', r'vridi', r'adjouffou', r'jean folly', r'port', r'petit bassam', r'vridi canal'],\n",
    "        # Treichville\n",
    "        'treichville': [r'arras', r'belleville', r'zone 3', r'avenue 16', r'avenue 8', r'avenue 12', r'avenue 21'],\n",
    "        # Yopougon\n",
    "        'yopougon': [r'yopougon carrefour chu', r'ananeraie', r'niangon', r'sid(é|e)ci', r'toits rouges', r'andokoi', r'selmer', r'maroc', r'kout(é|e)', r'wassakara', r'sicogi', r'gesco', r'yopougon cite verte', r'yopougon cité verte', r'yopougon academie', r'yopougon académie']\n",
    "    }\n",
    "\n",
    " \n",
    "    def process_location_info(df):\n",
    "        # Fonction pour extraire les informations de localisation (grappe, région, ville)\n",
    "        def extract_location_info(text):\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            grappe, region, ville, commune, quartier = None, None, None, None, None\n",
    "\n",
    "            # 1. Recherche du quartier et déduction de la commune, de la ville, et de la région pour Abidjan\n",
    "            for commune_key, quartiers in commune_to_quartier.items():\n",
    "                for quartier_key in quartiers:\n",
    "                    quartier_tokens = re.findall(r'\\b\\w+\\b', quartier_key.lower())\n",
    "                    if all(token in tokens for token in quartier_tokens):\n",
    "                        quartier = quartier_key\n",
    "                        commune = commune_key\n",
    "                        ville = 'abidjan'\n",
    "                        region = 'abidjan'\n",
    "                        grappe = 'abidjan'\n",
    "                        return grappe, region, ville, commune, quartier\n",
    "\n",
    "            # 2. Recherche de la commune et déduction de la ville et de la région pour Abidjan\n",
    "            for commune_key in ville_to_commune['abidjan']:\n",
    "                commune_tokens = re.findall(r'\\b\\w+\\b', commune_key.lower())\n",
    "                if all(token in tokens for token in commune_tokens):\n",
    "                    commune = commune_key\n",
    "                    ville = 'abidjan'\n",
    "                    region = 'abidjan'\n",
    "                    grappe = 'abidjan'\n",
    "                    return grappe, region, ville, commune, None\n",
    "\n",
    "            # 3. Recherche de la ville et déduction de la région\n",
    "            for region_key, villes in region_to_ville.items():\n",
    "                for ville_key in villes:\n",
    "                    ville_tokens = re.findall(r'\\b\\w+\\b', ville_key.lower())\n",
    "                    if all(token in tokens for token in ville_tokens):\n",
    "                        ville = ville_key\n",
    "                        region = region_key\n",
    "                        for grappe_key, regions in grappe_to_region.items():\n",
    "                            if region in regions:\n",
    "                                grappe = grappe_key\n",
    "                        return grappe, region, ville, None, None\n",
    "\n",
    "            # Retourner None pour toutes les valeurs si rien n'a été trouvé\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        # Appliquer la fonction d'extraction sur chaque ligne du DataFrame\n",
    "        df['grappe'], df['region'], df['ville'], df['commune'], df['quartier'] = zip(*df.apply(\n",
    "            lambda row: extract_location_info(f\"{row['title']} {row['localisation']} {row['description']}\"), axis=1))\n",
    "\n",
    "        # Fonction pour corriger la commune à partir de la description\n",
    "        def extract_commune_from_description(description):\n",
    "            tokens = re.findall(r'\\b\\w+\\b', description.lower())\n",
    "            for ville, communes in ville_to_commune.items():\n",
    "                for commune in communes:\n",
    "                    commune_tokens = re.findall(r'\\b\\w+\\b', commune.lower())\n",
    "                    if all(token in tokens for token in commune_tokens):\n",
    "                        return commune\n",
    "            return None\n",
    "\n",
    "        # Mise à jour de la commune si elle est vide et que la description peut en fournir une\n",
    "        df['commune'] = df.apply(lambda row: extract_commune_from_description(row['description']) if pd.isna(row['commune']) else row['commune'], axis=1)\n",
    "\n",
    "        # Fonction pour corriger la commune à partir du quartier\n",
    "        def extract_commune_from_quartier(row):\n",
    "            quartier = row['quartier']\n",
    "            commune = row['commune']\n",
    "            if pd.notna(quartier) and pd.isna(commune):\n",
    "                for commune_key, quartiers in commune_to_quartier.items():\n",
    "                    quartier_tokens = re.findall(r'\\b\\w+\\b', quartier.lower())\n",
    "                    for quartier_key in quartiers:\n",
    "                        quartier_key_tokens = re.findall(r'\\b\\w+\\b', quartier_key.lower())\n",
    "                        if all(token in quartier_tokens for token in quartier_key_tokens):\n",
    "                            return commune_key\n",
    "            return commune\n",
    "\n",
    "        # Appliquer la fonction pour corriger la commune à partir du quartier\n",
    "        df['commune'] = df.apply(extract_commune_from_quartier, axis=1)\n",
    "\n",
    "        # Fonction pour corriger la ville, la région, et la grappe à partir de la localisation si la commune est vide\n",
    "        def correct_ville_region_grappe_from_localisation(row):\n",
    "            localisation = row['localisation']\n",
    "            commune = row['commune']\n",
    "            ville = row['ville']\n",
    "            region = row['region']\n",
    "            grappe = row['grappe']\n",
    "            if pd.isna(commune) and pd.notna(localisation):\n",
    "                tokens = re.findall(r'\\b\\w+\\b', localisation.lower())\n",
    "                for region_key, villes in region_to_ville.items():\n",
    "                    for ville_key in villes:\n",
    "                        ville_tokens = re.findall(r'\\b\\w+\\b', ville_key.lower())\n",
    "                        if all(token in tokens for token in ville_tokens):\n",
    "                            ville = ville_key\n",
    "                            region = region_key\n",
    "                            for grappe_key, regions in grappe_to_region.items():\n",
    "                                if region in regions:\n",
    "                                    grappe = grappe_key\n",
    "                            return ville, region, grappe\n",
    "            return ville, region, grappe\n",
    "\n",
    "        # Appliquer la correction ville/région/grappe si commune est vide\n",
    "        df[['ville', 'region', 'grappe']] = df.apply(lambda row: pd.Series(correct_ville_region_grappe_from_localisation(row)), axis=1)\n",
    "\n",
    "        # Fonction pour corriger le quartier à partir de la concaténation de localisation et description si quartier est vide\n",
    "        def correct_quartier_from_localisation_description(row):\n",
    "            localisation = row['localisation']\n",
    "            description = row['description']\n",
    "            quartier = row['quartier']\n",
    "            commune = row['commune']\n",
    "            if pd.isna(quartier) and (pd.notna(localisation) or pd.notna(description)) and pd.notna(commune):\n",
    "                combined_text = f\"{localisation} {description}\".lower()\n",
    "                tokens = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "                if commune in commune_to_quartier:\n",
    "                    for quartier_key in commune_to_quartier[commune]:\n",
    "                        quartier_tokens = re.findall(r'\\b\\w+\\b', quartier_key.lower())\n",
    "                        if all(token in tokens for token in quartier_tokens):\n",
    "                            return quartier_key\n",
    "            return quartier\n",
    "\n",
    "        # Appliquer la correction du quartier si nécessaire\n",
    "        df['quartier'] = df.apply(lambda row: correct_quartier_from_localisation_description(row), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Appliquer la fonction sur le DataFrame\n",
    "    df = process_location_info(df)\n",
    "\n",
    "\n",
    "    # Afficher le nombre de lignes restantes\n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"2.8.\tCorrection du type d’immobilier à partir de la variable « description » \")\n",
    "\n",
    "    if 'type_immobilier' in df.columns:\n",
    "        df['type_immobilier'] = pd.NA\n",
    "\n",
    "    # Fonction pour déterminer le type d'immobilier\n",
    "    def classify_immobilier(row):\n",
    "        text = str(row['description']) if pd.notna(row['description']) else ''\n",
    "        text2 = str(row['title']) if pd.notna(row['title']) else ''\n",
    "        \n",
    "        nb_pieces = row['nb_pieces']\n",
    "        \n",
    "        # Vérifier si nb_pieces n'est pas manquant\n",
    "        if pd.notna(nb_pieces) and nb_pieces == 1:\n",
    "            return 'studio'\n",
    "        \n",
    "        # Identifier d'autres types d'immobilier à partir de la description (text)\n",
    "        if re.search(r'duplex', text, re.IGNORECASE):\n",
    "            return 'duplex'\n",
    "        if re.search(r'villa', text, re.IGNORECASE):\n",
    "            return 'villa'\n",
    "        if re.search(r'\\b(appartement|penthouse)\\b', text, re.IGNORECASE):\n",
    "            return 'appartement'\n",
    "        \n",
    "        # Si non trouvé dans la description, chercher dans le titre (text2)\n",
    "        if re.search(r'duplex', text2, re.IGNORECASE):\n",
    "            return 'duplex'\n",
    "        if re.search(r'villa', text2, re.IGNORECASE):\n",
    "            return 'villa'\n",
    "        if re.search(r'\\b(appartement|penthouse)\\b', text2, re.IGNORECASE):\n",
    "            return 'appartement'\n",
    "        \n",
    "        # Par défaut, classifier comme 'maison'\n",
    "        return 'maison'\n",
    "\n",
    "    # Appliquer la fonction pour créer la colonne 'type_immobilier'\n",
    "    if 'type d\\'immobilier' in df.columns:\n",
    "        df['type_immobilier'] = df.apply(classify_immobilier, axis=1)\n",
    "\n",
    "        # Afficher quelques lignes pour vérification avant suppression\n",
    "        print(df[['type d\\'immobilier', 'type_immobilier']].head())\n",
    "        \n",
    "        # Supprimer la colonne 'type d\\'immobilier' si tout est correct\n",
    "        df.drop(columns=['type d\\'immobilier'], inplace=True)\n",
    "\n",
    "    df = df.dropna(subset=['type_immobilier'])\n",
    "\n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "\n",
    "    print(\" 2.9.\tCréation de variables de classification des variétés (eau, électricité, type de matériaux, toilette interne ou externe, cours commune, quartiers_chics, nb_pieces_classes)\")\n",
    "\n",
    "    # Fonction pour déterminer si les toilettes sont internes\n",
    "    def has_toilettes_internes(description):\n",
    "        return 0 if re.search(r\"(toilette|wc|douche|salle d'eau) externe\", description, re.IGNORECASE) else 1\n",
    "\n",
    "    # Fonction pour déterminer si la cour est commune\n",
    "    def has_cours_commune(description):\n",
    "        return 1 if re.search(r\"cours commune\", description, re.IGNORECASE) else 0\n",
    "\n",
    "    # Appliquer les fonctions pour créer les nouvelles variables\n",
    "    df['Avec_eau'] = 1\n",
    "    df['Avec_electricite'] = 1\n",
    "    df['Materiaux_en_dur'] = 1\n",
    "    df['Toilettes_internes'] = df['description'].apply(has_toilettes_internes)\n",
    "    df['cours_commune'] = df['description'].apply(has_cours_commune)\n",
    "\n",
    "\n",
    "    # Définir la liste des quartiers chics\n",
    "    quartiers_chics = [\n",
    "        r'angré', \n",
    "        r'riviera', \n",
    "        r'riviera 1', \n",
    "        r'riviera 2', \n",
    "        r'riviera 3', \n",
    "        r'riviera 4', \n",
    "        'palmeraie', \n",
    "        'deux plateaux', \n",
    "        'danga', \n",
    "        'mermoz', \n",
    "        r'vallon', \n",
    "        'cité des arts', \n",
    "        'zone 4', \n",
    "        'biétry', \n",
    "        'hibiscus', \n",
    "        'résidentiel', \n",
    "        'plateau',\n",
    "        'le plateau',\n",
    "        'ambassade',\n",
    "        'golf',\n",
    "        'faya',\n",
    "        'residentiel'\n",
    "    ]\n",
    "\n",
    "    # Fonction pour déterminer si la localisation est dans un quartier chic\n",
    "    def is_quartier_chic(quartier):\n",
    "        if pd.isna(quartier): \n",
    "            return 0\n",
    "        quartier = str(quartier)\n",
    "        for quartier1 in quartiers_chics:\n",
    "            if re.search(quartier1, quartier, re.IGNORECASE):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    # Appliquer la fonction pour créer la variable 'quartier_chic'\n",
    "    df['quartier_chic'] = df['quartier'].apply(is_quartier_chic)\n",
    "\n",
    "    # Fonction pour classifier le nombre de pièces\n",
    "    def classify_nb_pieces(nb_pieces):\n",
    "        if pd.isna(nb_pieces):\n",
    "            return None\n",
    "        elif nb_pieces == 1:\n",
    "            return '1 piece'\n",
    "        elif 2 <= nb_pieces <= 3:\n",
    "            return '2 et 3 pieces'\n",
    "        elif nb_pieces >= 4:\n",
    "            return '4 pieces et plus'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Appliquer la fonction pour créer la colonne 'nb_pieces_classe'\n",
    "    df['nb_pieces_classe'] = df['nb_pieces'].apply(classify_nb_pieces)\n",
    "\n",
    "\n",
    "    print(f'Restants : {len(df)}')\n",
    "\n",
    "\n",
    "\n",
    "    # Définir les dictionnaires de mots regroupés\n",
    "    commodities = {\n",
    "        'wifi': [r'wifi', r'internet', r'connexion', r'fibre optique'],\n",
    "        'jardin': [r'jardin(\\.locat\\d+)?'],\n",
    "        'meubler': [r'meubl(er|é|e)'],\n",
    "        'climatisation': [r'climat(isé|isation|iseur)', r'air conditionner', r'split'],\n",
    "        'garage': [r'garage'],\n",
    "        'balcon': [r'balcon'],\n",
    "        'sécuriser': [r'sécur(is|it|is)(er|e|é)'],\n",
    "        'étage': [r'étage'],\n",
    "        'parking': [r'parking'],\n",
    "        'douche': [r'douche'],\n",
    "        'séjour': [r'séjour', r'sejour'],\n",
    "        'équiper': [r'équip(er|é|e|ée|ement)'],\n",
    "        'placard': [r'placard'],\n",
    "        'salle': [r'salle'],\n",
    "        'manger': [r'manger'],\n",
    "        'bureau': [r'bureau'],\n",
    "        'piscine': [r'piscine'],\n",
    "        'gardien': [r'gardi(en|en)', r'garde'],\n",
    "        'électrogène': [r'électrogène'],\n",
    "        'four': [r'four'],\n",
    "        'cinéma': [r'cinéma', r'netflix'],\n",
    "        'ascenseur': [r'ascenseur'],\n",
    "        'double vitrage': [r'double vitrage'],\n",
    "        'terrasse': [r'terrasse'],\n",
    "        'toilette': [r'toilette', r'wc'],\n",
    "        'buanderie': [r'buanderie'],\n",
    "        'chauffe': [r'chauff(e|e-eau)'],\n",
    "        'cuisine': [r'cuisine']\n",
    "    }\n",
    "\n",
    "    natural_externalities = {\n",
    "        'bord_de_mer': [r'plage', r'mer', r'océan'],\n",
    "        'forêt': [r'for(ê|e)t'],\n",
    "        'parc': [r'parc'],\n",
    "        'montagne': [r'montagne'],\n",
    "        'rivière': [r'rivi(è|e)re'],\n",
    "        'lac': [r'lac']\n",
    "    }\n",
    "\n",
    "    artificial_externalities = {\n",
    "        'zone_commerciale': [r'centre commercial', r'marché', r'zone commerciale'],\n",
    "        'pharmacie': [r'pharmacie'],\n",
    "        'restaurant': [r'restaurant', r'resto'],\n",
    "        'bar': [r'bar', r'maquis'],\n",
    "        'ambassade': [r'ambassade'],\n",
    "        'école': [r'écol(e|es)', r'crèch(e|es)', r'lycé(e|es)', r'universit(é|és)'],\n",
    "        'hôpital': [r'hôpital', r'clinique', r'dispensaire', r'CHU', r'CHR', r'medecin']\n",
    "    }\n",
    "\n",
    "    # Fonction pour extraire les informations de la description\n",
    "    def extract_description_info(df, grouped_words):\n",
    "        def extract_info(description):\n",
    "            if not isinstance(description, str):\n",
    "                return {key: 0 for key in grouped_words}\n",
    "            \n",
    "            doc = nlp(description)\n",
    "            info = {key: 0 for key in grouped_words}\n",
    "            \n",
    "            negation = False\n",
    "            for token in doc:\n",
    "                if token.dep_ == 'neg':\n",
    "                    negation = True\n",
    "                for key, patterns in grouped_words.items():\n",
    "                    for pattern in patterns:\n",
    "                        if re.search(pattern, token.lemma_):\n",
    "                            info[key] = 0 if negation else 1\n",
    "                            negation = False\n",
    "            \n",
    "            return info\n",
    "        \n",
    "        description_info = df['description'].apply(extract_info)\n",
    "        \n",
    "        for key in grouped_words.keys():\n",
    "            df[key] = description_info.apply(lambda x: x[key])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Appliquer les fonctions pour extraire les informations des commodités et externalités\n",
    "    df = extract_description_info(df, commodities)\n",
    "    df = extract_description_info(df, natural_externalities)\n",
    "    df = extract_description_info(df, artificial_externalities)\n",
    "\n",
    "    # Créer les variables 'commodités', 'externalités_naturelles' et 'externalités_artificielles'\n",
    "    df['commodités'] = df[list(commodities.keys())].max(axis=1)\n",
    "    df['externalités_naturelles'] = df[list(natural_externalities.keys())].max(axis=1)\n",
    "    df['externalités_artificielles'] = df[list(artificial_externalities.keys())].max(axis=1)\n",
    "\n",
    "\n",
    "    # Exporter le dataframe final avec les modifications\n",
    "    import platform\n",
    "    if platform.system() == 'Windows':\n",
    "        output_path_final = f'D:\\\\Bureau\\\\MemoiresStages\\\\Travaux_techniques\\\\Traitements\\\\Datasets_pretraites\\\\pretraitees\\\\{mois}_pretraitee.xlsx'\n",
    "    else:\n",
    "        output_path_final = f'/mnt/d/Bureau/MemoiresStages/Travaux_techniques/Traitements/Datasets_pretraites/{mois}_pretraitee.xlsx'\n",
    "\n",
    "    df.to_excel(output_path_final, index=False)\n",
    "\n",
    "    print(f\"Fichier final avec les modifications sauvegardé sous {output_path_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
